{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uy_22Q-1ArXV",
    "outputId": "4853b270-bd84-433e-ca7c-6621feb29327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKWG-WtJA57T"
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"Put your token here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "92ff4a467f1e48de8c9cc891853af457",
      "3d268814ff3f4685a3a29c281d11e71b",
      "5c9f0a73169f47a1841d7ed70175f80b",
      "26e9aafa04e54383b4682b75cf67d3df",
      "b7d9cf86760c41fbacae8ed644357758",
      "7940d250af4246b2b1d42f3b7cf3f3a1",
      "e21599a6e38844e39915c89cfaeaed95",
      "2e0552c12c0e4598ba8c6eaf2745e936",
      "e9ff2ad176614d9fbbe83347e86a313b",
      "1befa314232048f2a7265c280248e789",
      "f1b194374a3240d8aab759ddef189ad6"
     ]
    },
    "id": "YMg-ahOVCTXH",
    "outputId": "13e30c0f-92ea-4761-9558-2fd359f51bc7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6dc5510ff248ba8fd8798660f45e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training corpus file from 10000 Wikipedia articles...\n",
      "Finished creating wiki_corpus.txt!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import itertools # We need this to limit our streaming dataset\n",
    "\n",
    "# Load the dataset\n",
    "wiki_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split='train', streaming=True)\n",
    "\n",
    "# We'll create one large text file to train our tokenizer on.\n",
    "# We'll start with 10,000 articles to keep the process fast for now.\n",
    "num_articles_to_use = 10000\n",
    "\n",
    "print(f\"Creating training corpus file from {num_articles_to_use} Wikipedia articles...\")\n",
    "\n",
    "with open(\"wiki_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Loop through the first 10,000 articles in the dataset\n",
    "    for article in itertools.islice(wiki_dataset, num_articles_to_use):\n",
    "        # Get the text content of the article\n",
    "        text_content = article['text']\n",
    "        # Write it to the file, followed by two newlines to separate articles\n",
    "        f.write(text_content + \"\\n\\n\")\n",
    "\n",
    "print(\"Finished creating wiki_corpus.txt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QYZAC-sSCYco"
   },
   "outputs": [],
   "source": [
    "# import regex as re\n",
    "# import requests\n",
    "\n",
    "# class Tokenizer:\n",
    "#     def __init__(self):\n",
    "#         self.merges = {}\n",
    "#         self.vocab = {}\n",
    "#         self.pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "#     def _get_pair_counts(self, tokens):\n",
    "#         counts = {}\n",
    "#         for i in range(len(tokens) - 1):\n",
    "#             pair = (tokens[i], tokens[i+1])\n",
    "#             counts[pair] = counts.get(pair, 0) + 1\n",
    "#         return sorted(((v, k) for k, v in counts.items()), reverse=True)\n",
    "\n",
    "#     def _merge(self, tokens, pair, new_idx):\n",
    "#         new_tokens = []\n",
    "#         i = 0\n",
    "#         while i < len(tokens):\n",
    "#             if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "#                 new_tokens.append(new_idx)\n",
    "#                 i += 2\n",
    "#             else:\n",
    "#                 new_tokens.append(tokens[i])\n",
    "#                 i += 1\n",
    "#         return new_tokens\n",
    "\n",
    "#     def train(self, text, num_merges, verbose=False):\n",
    "#         print(\"Starting training...\")\n",
    "#         text_chunks = self.pattern.findall(text)\n",
    "#         tokens = []\n",
    "#         for chunk in text_chunks:\n",
    "#             tokens.extend(list(chunk.encode(\"utf-8\")))\n",
    "\n",
    "#         merges = {}\n",
    "#         for i in range(num_merges):\n",
    "#             pair_counts = self._get_pair_counts(tokens)\n",
    "#             if not pair_counts:\n",
    "#                 break\n",
    "\n",
    "#             top_pair = pair_counts[0][1]\n",
    "#             new_idx = 256 + i\n",
    "#             merges[top_pair] = new_idx\n",
    "#             tokens = self._merge(tokens, top_pair, new_idx)\n",
    "#             if verbose and (i + 1) % 50 == 0:\n",
    "#                 print(f\"  Merge {i+1}/{num_merges} completed...\")\n",
    "\n",
    "#         self.merges = merges\n",
    "#         self._build_vocab()\n",
    "#         print(\"Training finished!\")\n",
    "\n",
    "#     def _build_vocab(self):\n",
    "#         self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "#         for (p1, p2), idx in self.merges.items():\n",
    "#             self.vocab[idx] = self.vocab[p1] + self.vocab[p2]\n",
    "\n",
    "#     def save(self, filepath=\"merge_rules.bpe\"):\n",
    "#         print(f\"Saving merge rules to {filepath}...\")\n",
    "#         with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "#             for (p1, p2) in self.merges:\n",
    "#                 f.write(f\"{p1} {p2}\\n\")\n",
    "#         print(\"Done.\")\n",
    "\n",
    "#     def load(self, filepath=\"merge_rules.bpe\"):\n",
    "#         print(f\"Loading merge rules from {filepath}...\")\n",
    "#         merges = {}\n",
    "#         with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "#             for i, line in enumerate(f):\n",
    "#                 p1, p2 = line.strip().split()\n",
    "#                 merges[(int(p1), int(p2))] = 256 + i\n",
    "#         self.merges = merges\n",
    "#         self._build_vocab()\n",
    "#         print(\"Tokenizer loaded.\")\n",
    "\n",
    "#     def encode(self, text):\n",
    "#         text_chunks = self.pattern.findall(text)\n",
    "#         tokens = []\n",
    "#         for chunk in text_chunks:\n",
    "#             tokens.extend(list(chunk.encode(\"utf-8\")))\n",
    "\n",
    "#         for pair, new_idx in self.merges.items():\n",
    "#             tokens = self._merge(tokens, pair, new_idx)\n",
    "#         return tokens\n",
    "\n",
    "#     def decode(self, ids):\n",
    "#         byte_chunk = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "#         return byte_chunk.decode(\"utf-8\", errors=\"replace\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GzW9O45Ftw8k"
   },
   "outputs": [],
   "source": [
    "# print(\"Dowloading text for training...\")\n",
    "# with open(\"wiki_corpus.txt\",'r') as file:\n",
    "#   text = file.read()\n",
    "# tokenizer = Tokenizer()\n",
    "# print(\"Training tokenizer...\")\n",
    "# tokenizer.train(text, num_merges=500, verbose=True)\n",
    "# tokenizer.save(\"my_tokenizer.bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tokenizers) (0.34.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.21.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pXnpNrA54Ay",
    "outputId": "f3487328-5aca-4f6a-8dea-b6824f2b046f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initialized. Starting training...\n",
      "\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Tokenizer saved to wiki-bpe-tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# Due to comuting time I have switched to default library which made it faster to train the tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "\n",
    "print(\"Tokenizer initialized. Starting training...\")\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train(files=[\"wiki_corpus.txt\"], trainer=trainer)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n",
    "tokenizer.save(\"wiki-bpe-tokenizer.json\")\n",
    "\n",
    "print(\"Tokenizer saved to wiki-bpe-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ggOxRdYAqwyw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # Linear layers to project inputs into V, K, Q spaces\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.query = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "\n",
    "        # Final fully connected layer to produce the output\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    # (This code goes inside the SelfAttention class)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get the number of training examples in the batch\n",
    "        N = query.shape[0]\n",
    "\n",
    "        # Get the length of the input sequences\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Pass inputs through the linear layers\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        query = self.query(query)\n",
    "\n",
    "        # Reshape the tensors to split them into multiple heads\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # --- The Core Attention Calculation ---\n",
    "        # Matrix multiply Queries and Keys to get attention scores\n",
    "        # query shape: (N, query_len, heads, head_dim)\n",
    "        # keys shape: (N, key_len, heads, head_dim)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [query, keys])\n",
    "\n",
    "        # Apply the mask, if one is provided\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax to get attention weights (probabilities)\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        # --- Apply Attention to Values ---\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, head_dim)\n",
    "        # out shape: (N, query_len, heads, head_dim)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        # Pass the result through the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vWYNtbcYsMLU"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # 1. The attention layer\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        # 2. Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # 3. The feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        # 4. Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "      # 1. Pass inputs through the self-attention layer\n",
    "      attention = self.attention(value, key, query, mask)\n",
    "\n",
    "      # 2. Add the output of attention to the original query (the skip connection)\n",
    "      #    and then pass it through the first normalization layer.\n",
    "      x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "      # 3. Pass the result through the feed-forward network\n",
    "      forward = self.feed_forward(x)\n",
    "\n",
    "      # 4. Add the output of the feed-forward network to its input (the second skip connection)\n",
    "      #    and then pass it through the second normalization layer.\n",
    "      out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hXnCLgcmslbt"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # The attention mask from the tokenizer is of shape (N, seq_length).\n",
    "        # We need to reshape it to (N, 1, 1, seq_length) to make it broadcastable\n",
    "        # with the attention scores tensor of shape (N, heads, query_len, key_len).\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Pass the data through each Transformer Block in the stack\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "x5xQybXqs4K7"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, encoder, vocab_size):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # This is the final layer that makes the word predictions\n",
    "        self.fc_out = nn.Linear(encoder.embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # First, get the context-rich output from the encoder\n",
    "        out = self.encoder(x, mask)\n",
    "\n",
    "        # Then, pass it through the final linear layer to get predictions\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "SOBrYSY6s5AJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.lines = []\n",
    "\n",
    "        # Get special token IDs once\n",
    "        self.mask_token_id = tokenizer.token_to_id(\"[MASK]\")\n",
    "        self.pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "        self.sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "        # Let the library handle padding and truncation\n",
    "        self.tokenizer.enable_padding(length=self.max_length, pad_id=self.pad_token_id)\n",
    "        self.tokenizer.enable_truncation(max_length=self.max_length)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.lines.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.lines[idx]\n",
    "        # The encoding object now contains ids, attention_mask, etc.\n",
    "        encoding = self.tokenizer.encode(line)\n",
    "\n",
    "        input_ids = torch.tensor(encoding.ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(encoding.attention_mask, dtype=torch.long)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Create a mask for tokens we can potentially mask.\n",
    "        # We don't want to mask special tokens like [CLS], [SEP], [PAD].\n",
    "        can_mask = (input_ids != self.cls_token_id) & \\\n",
    "                   (input_ids != self.sep_token_id) & \\\n",
    "                   (input_ids != self.pad_token_id)\n",
    "\n",
    "        # Determine which tokens to mask (15% of the maskable tokens)\n",
    "        mask_indices = torch.where(can_mask & (torch.rand(input_ids.shape) < 0.15))\n",
    "\n",
    "        for i in mask_indices[0]:\n",
    "            prob = random.random()\n",
    "            if prob < 0.8:\n",
    "                # 80% of the time: Replace with [MASK]\n",
    "                input_ids[i] = self.mask_token_id\n",
    "            elif prob < 0.9:\n",
    "                # 10% of the time: Replace with a random token\n",
    "                input_ids[i] = random.randint(0, self.vocab_size - 1)\n",
    "            # else 10% of the time: Keep the original token (do nothing)\n",
    "\n",
    "        # We only compute loss on the tokens we masked.\n",
    "        # Set all other labels to -100 so they are ignored by the loss function.\n",
    "        # Create a boolean mask for all positions that were NOT masked.\n",
    "        non_masked_indices = torch.ones(labels.shape, dtype=torch.bool)\n",
    "        non_masked_indices[mask_indices] = False\n",
    "\n",
    "        # Use this boolean mask to set the labels at these positions to -100.\n",
    "        labels[non_masked_indices] = -100\n",
    "\n",
    "        return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqRvThdktgfr",
    "outputId": "2638c47d-1f9d-494e-f55e-17e995d3b8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Epoch 1/5 ---\n",
      "  Batch 100/17006, Loss: 8.6460\n",
      "  Batch 200/17006, Loss: 8.1178\n",
      "  Batch 300/17006, Loss: 7.7455\n",
      "  Batch 400/17006, Loss: 7.8862\n",
      "  Batch 500/17006, Loss: 8.2632\n",
      "  Batch 600/17006, Loss: 8.1931\n",
      "  Batch 700/17006, Loss: 7.9859\n",
      "  Batch 800/17006, Loss: 8.2194\n",
      "  Batch 900/17006, Loss: 7.9190\n",
      "  Batch 1000/17006, Loss: 7.4765\n",
      "  Batch 1100/17006, Loss: 7.5844\n",
      "  Batch 1200/17006, Loss: 7.6251\n",
      "  Batch 1300/17006, Loss: 7.5998\n",
      "  Batch 1400/17006, Loss: 8.0714\n",
      "  Batch 1500/17006, Loss: 7.6162\n",
      "  Batch 1600/17006, Loss: 7.4739\n",
      "  Batch 1700/17006, Loss: 7.9230\n",
      "  Batch 1800/17006, Loss: 8.2018\n",
      "  Batch 1900/17006, Loss: 8.2211\n",
      "  Batch 2000/17006, Loss: 7.4853\n",
      "  Batch 2100/17006, Loss: 7.7598\n",
      "  Batch 2200/17006, Loss: 8.0013\n",
      "  Batch 2300/17006, Loss: 7.9136\n",
      "  Batch 2400/17006, Loss: 7.3744\n",
      "  Batch 2500/17006, Loss: 7.7510\n",
      "  Batch 2600/17006, Loss: 7.9412\n",
      "  Batch 2700/17006, Loss: 7.7274\n",
      "  Batch 2800/17006, Loss: 7.3608\n",
      "  Batch 2900/17006, Loss: 7.4658\n",
      "  Batch 3000/17006, Loss: 7.3588\n",
      "  Batch 3100/17006, Loss: 7.7593\n",
      "  Batch 3200/17006, Loss: 8.1740\n",
      "  Batch 3300/17006, Loss: 7.6567\n",
      "  Batch 3400/17006, Loss: 7.8393\n",
      "  Batch 3500/17006, Loss: 7.5200\n",
      "  Batch 3600/17006, Loss: 7.7187\n",
      "  Batch 3700/17006, Loss: 7.3961\n",
      "  Batch 3800/17006, Loss: 6.9878\n",
      "  Batch 3900/17006, Loss: 8.0713\n",
      "  Batch 4000/17006, Loss: 7.4314\n",
      "  Batch 4100/17006, Loss: 7.5874\n",
      "  Batch 4200/17006, Loss: 7.0247\n",
      "  Batch 4300/17006, Loss: 7.3592\n",
      "  Batch 4400/17006, Loss: 7.2243\n",
      "  Batch 4500/17006, Loss: 7.5905\n",
      "  Batch 4600/17006, Loss: 8.0145\n",
      "  Batch 4700/17006, Loss: 7.8828\n",
      "  Batch 4800/17006, Loss: 7.4170\n",
      "  Batch 4900/17006, Loss: 7.7417\n",
      "  Batch 5000/17006, Loss: 6.9949\n",
      "  Batch 5100/17006, Loss: 7.6759\n",
      "  Batch 5200/17006, Loss: 7.1809\n",
      "  Batch 5300/17006, Loss: 7.7395\n",
      "  Batch 5400/17006, Loss: 7.8768\n",
      "  Batch 5500/17006, Loss: 7.1685\n",
      "  Batch 5600/17006, Loss: 7.8880\n",
      "  Batch 5700/17006, Loss: 7.3539\n",
      "  Batch 5800/17006, Loss: 7.6338\n",
      "  Batch 5900/17006, Loss: 7.7393\n",
      "  Batch 6000/17006, Loss: 7.3564\n",
      "  Batch 6100/17006, Loss: 7.3465\n",
      "  Batch 6200/17006, Loss: 7.7021\n",
      "  Batch 6300/17006, Loss: 7.4496\n",
      "  Batch 6400/17006, Loss: 7.2054\n",
      "  Batch 6500/17006, Loss: 7.4790\n",
      "  Batch 6600/17006, Loss: 8.0468\n",
      "  Batch 6700/17006, Loss: 8.1821\n",
      "  Batch 6800/17006, Loss: 6.9399\n",
      "  Batch 6900/17006, Loss: 7.6593\n",
      "  Batch 7000/17006, Loss: 7.0804\n",
      "  Batch 7100/17006, Loss: 7.5332\n",
      "  Batch 7200/17006, Loss: 7.2780\n",
      "  Batch 7300/17006, Loss: 7.7391\n",
      "  Batch 7400/17006, Loss: 7.3648\n",
      "  Batch 7500/17006, Loss: 7.6967\n",
      "  Batch 7600/17006, Loss: 7.5945\n",
      "  Batch 7700/17006, Loss: 7.5299\n",
      "  Batch 7800/17006, Loss: 7.7555\n",
      "  Batch 7900/17006, Loss: 7.3327\n",
      "  Batch 8000/17006, Loss: 7.7884\n",
      "  Batch 8100/17006, Loss: 7.5901\n",
      "  Batch 8200/17006, Loss: 7.3230\n",
      "  Batch 8300/17006, Loss: 7.5497\n",
      "  Batch 8400/17006, Loss: 7.4560\n",
      "  Batch 8500/17006, Loss: 7.1712\n",
      "  Batch 8600/17006, Loss: 7.8131\n",
      "  Batch 8700/17006, Loss: 7.3869\n",
      "  Batch 8800/17006, Loss: 7.5977\n",
      "  Batch 8900/17006, Loss: 8.1012\n",
      "  Batch 9000/17006, Loss: 7.7412\n",
      "  Batch 9100/17006, Loss: 7.5563\n",
      "  Batch 9200/17006, Loss: 7.3527\n",
      "  Batch 9300/17006, Loss: 7.1176\n",
      "  Batch 9400/17006, Loss: 7.4267\n",
      "  Batch 9500/17006, Loss: 7.7334\n",
      "  Batch 9600/17006, Loss: 7.0792\n",
      "  Batch 9700/17006, Loss: 7.5139\n",
      "  Batch 9800/17006, Loss: 7.0280\n",
      "  Batch 9900/17006, Loss: 7.6650\n",
      "  Batch 10000/17006, Loss: 7.2464\n",
      "  Batch 10100/17006, Loss: 7.3396\n",
      "  Batch 10200/17006, Loss: 7.5273\n",
      "  Batch 10300/17006, Loss: 7.2102\n",
      "  Batch 10400/17006, Loss: 7.3938\n",
      "  Batch 10500/17006, Loss: 7.2966\n",
      "  Batch 10600/17006, Loss: 7.1216\n",
      "  Batch 10700/17006, Loss: 7.4649\n",
      "  Batch 10800/17006, Loss: 7.1439\n",
      "  Batch 10900/17006, Loss: 7.2812\n",
      "  Batch 11000/17006, Loss: 7.2783\n",
      "  Batch 11100/17006, Loss: 7.3226\n",
      "  Batch 11200/17006, Loss: 7.0259\n",
      "  Batch 11300/17006, Loss: 7.4816\n",
      "  Batch 11400/17006, Loss: 7.1742\n",
      "  Batch 11500/17006, Loss: 7.6067\n",
      "  Batch 11600/17006, Loss: 7.1873\n",
      "  Batch 11700/17006, Loss: 7.4297\n",
      "  Batch 11800/17006, Loss: 7.2764\n",
      "  Batch 11900/17006, Loss: 7.0976\n",
      "  Batch 12000/17006, Loss: 7.3967\n",
      "  Batch 12100/17006, Loss: 7.5497\n",
      "  Batch 12200/17006, Loss: 7.3906\n",
      "  Batch 12300/17006, Loss: 7.1370\n",
      "  Batch 12400/17006, Loss: 7.4989\n",
      "  Batch 12500/17006, Loss: 7.5654\n",
      "  Batch 12600/17006, Loss: 7.5624\n",
      "  Batch 12700/17006, Loss: 7.6543\n",
      "  Batch 12800/17006, Loss: 7.0498\n",
      "  Batch 12900/17006, Loss: 7.6169\n",
      "  Batch 13000/17006, Loss: 7.0896\n",
      "  Batch 13100/17006, Loss: 7.0968\n",
      "  Batch 13200/17006, Loss: 7.8610\n",
      "  Batch 13300/17006, Loss: 7.6316\n",
      "  Batch 13400/17006, Loss: 7.2568\n",
      "  Batch 13500/17006, Loss: 7.6036\n",
      "  Batch 13600/17006, Loss: 7.5628\n",
      "  Batch 13700/17006, Loss: 7.6601\n",
      "  Batch 13800/17006, Loss: 7.0633\n",
      "  Batch 13900/17006, Loss: 6.7587\n",
      "  Batch 14000/17006, Loss: 7.0017\n",
      "  Batch 14100/17006, Loss: 7.6308\n",
      "  Batch 14200/17006, Loss: 7.5156\n",
      "  Batch 14300/17006, Loss: 7.1340\n",
      "  Batch 14400/17006, Loss: 7.1403\n",
      "  Batch 14500/17006, Loss: 7.1246\n",
      "  Batch 14600/17006, Loss: 7.5802\n",
      "  Batch 14700/17006, Loss: 7.1737\n",
      "  Batch 14800/17006, Loss: 6.4793\n",
      "  Batch 14900/17006, Loss: 6.9890\n",
      "  Batch 15000/17006, Loss: 7.1155\n",
      "  Batch 15100/17006, Loss: 7.8636\n",
      "  Batch 15200/17006, Loss: 7.5536\n",
      "  Batch 15300/17006, Loss: 7.5623\n",
      "  Batch 15400/17006, Loss: 7.0720\n",
      "  Batch 15500/17006, Loss: 7.5480\n",
      "  Batch 15600/17006, Loss: 7.4431\n",
      "  Batch 15700/17006, Loss: 7.2523\n",
      "  Batch 15800/17006, Loss: 6.6622\n",
      "  Batch 15900/17006, Loss: 7.4848\n",
      "  Batch 16000/17006, Loss: 6.9525\n",
      "  Batch 16100/17006, Loss: 7.1982\n",
      "  Batch 16200/17006, Loss: 7.4174\n",
      "  Batch 16300/17006, Loss: 7.5267\n",
      "  Batch 16400/17006, Loss: 7.2721\n",
      "  Batch 16500/17006, Loss: 6.6872\n",
      "  Batch 16600/17006, Loss: 7.4143\n",
      "  Batch 16700/17006, Loss: 6.8716\n",
      "  Batch 16800/17006, Loss: 7.4433\n",
      "  Batch 16900/17006, Loss: 6.9090\n",
      "  Batch 17000/17006, Loss: 7.3996\n",
      "Epoch 1 finished. Average Loss: 7.5299\n",
      "\n",
      "--- Epoch 2/5 ---\n",
      "  Batch 100/17006, Loss: 7.4382\n",
      "  Batch 200/17006, Loss: 7.0267\n",
      "  Batch 300/17006, Loss: 7.4140\n",
      "  Batch 400/17006, Loss: 7.6930\n",
      "  Batch 500/17006, Loss: 7.2088\n",
      "  Batch 600/17006, Loss: 7.1136\n",
      "  Batch 700/17006, Loss: 7.3163\n",
      "  Batch 800/17006, Loss: 7.6239\n",
      "  Batch 900/17006, Loss: 8.4539\n",
      "  Batch 1000/17006, Loss: 7.4845\n",
      "  Batch 1100/17006, Loss: 7.4351\n",
      "  Batch 1200/17006, Loss: 7.5157\n",
      "  Batch 1300/17006, Loss: 7.0983\n",
      "  Batch 1400/17006, Loss: 7.2570\n",
      "  Batch 1500/17006, Loss: 6.9206\n",
      "  Batch 1600/17006, Loss: 7.1620\n",
      "  Batch 1700/17006, Loss: 7.5627\n",
      "  Batch 1800/17006, Loss: 7.3395\n",
      "  Batch 1900/17006, Loss: 7.1341\n",
      "  Batch 2000/17006, Loss: 7.0243\n",
      "  Batch 2100/17006, Loss: 7.5039\n",
      "  Batch 2200/17006, Loss: 7.4414\n",
      "  Batch 2300/17006, Loss: 7.0508\n",
      "  Batch 2400/17006, Loss: 6.9259\n",
      "  Batch 2500/17006, Loss: 7.4949\n",
      "  Batch 2600/17006, Loss: 7.3449\n",
      "  Batch 2700/17006, Loss: 6.6135\n",
      "  Batch 2800/17006, Loss: 7.1663\n",
      "  Batch 2900/17006, Loss: 7.2820\n",
      "  Batch 3000/17006, Loss: 6.7938\n",
      "  Batch 3100/17006, Loss: 7.0896\n",
      "  Batch 3200/17006, Loss: 7.6776\n",
      "  Batch 3300/17006, Loss: 7.1252\n",
      "  Batch 3400/17006, Loss: 7.3726\n",
      "  Batch 3500/17006, Loss: 6.3642\n",
      "  Batch 3600/17006, Loss: 7.1127\n",
      "  Batch 3700/17006, Loss: 7.1433\n",
      "  Batch 3800/17006, Loss: 7.5359\n",
      "  Batch 3900/17006, Loss: 7.4365\n",
      "  Batch 4000/17006, Loss: 6.6493\n",
      "  Batch 4100/17006, Loss: 6.9433\n",
      "  Batch 4200/17006, Loss: 7.6349\n",
      "  Batch 4300/17006, Loss: 6.9575\n",
      "  Batch 4400/17006, Loss: 7.3161\n",
      "  Batch 4500/17006, Loss: 7.3589\n",
      "  Batch 4600/17006, Loss: 7.1643\n",
      "  Batch 4700/17006, Loss: 6.7457\n",
      "  Batch 4800/17006, Loss: 7.1037\n",
      "  Batch 4900/17006, Loss: 7.2406\n",
      "  Batch 5000/17006, Loss: 7.2793\n",
      "  Batch 5100/17006, Loss: 7.0920\n",
      "  Batch 5200/17006, Loss: 7.5216\n",
      "  Batch 5300/17006, Loss: 6.9990\n",
      "  Batch 5400/17006, Loss: 7.0124\n",
      "  Batch 5500/17006, Loss: 7.2169\n",
      "  Batch 5600/17006, Loss: 7.5283\n",
      "  Batch 5700/17006, Loss: 6.8668\n",
      "  Batch 5800/17006, Loss: 7.0034\n",
      "  Batch 5900/17006, Loss: 6.9272\n",
      "  Batch 6000/17006, Loss: 7.2365\n",
      "  Batch 6100/17006, Loss: 7.3446\n",
      "  Batch 6200/17006, Loss: 7.4844\n",
      "  Batch 6300/17006, Loss: 6.9611\n",
      "  Batch 6400/17006, Loss: 7.5878\n",
      "  Batch 6500/17006, Loss: 7.1623\n",
      "  Batch 6600/17006, Loss: 6.8714\n",
      "  Batch 6700/17006, Loss: 6.6517\n",
      "  Batch 6800/17006, Loss: 7.1309\n",
      "  Batch 6900/17006, Loss: 7.4007\n",
      "  Batch 7000/17006, Loss: 6.9584\n",
      "  Batch 7100/17006, Loss: 7.2070\n",
      "  Batch 7200/17006, Loss: 7.2002\n",
      "  Batch 7300/17006, Loss: 7.1533\n",
      "  Batch 7400/17006, Loss: 7.2610\n",
      "  Batch 7500/17006, Loss: 7.2692\n",
      "  Batch 7600/17006, Loss: 7.0206\n",
      "  Batch 7700/17006, Loss: 7.0946\n",
      "  Batch 7800/17006, Loss: 7.0004\n",
      "  Batch 7900/17006, Loss: 6.9307\n",
      "  Batch 8000/17006, Loss: 7.0111\n",
      "  Batch 8100/17006, Loss: 7.2811\n",
      "  Batch 8200/17006, Loss: 7.1313\n",
      "  Batch 8300/17006, Loss: 7.3356\n",
      "  Batch 8400/17006, Loss: 6.9973\n",
      "  Batch 8500/17006, Loss: 6.7666\n",
      "  Batch 8600/17006, Loss: 7.5519\n",
      "  Batch 8700/17006, Loss: 7.6147\n",
      "  Batch 8800/17006, Loss: 7.2929\n",
      "  Batch 8900/17006, Loss: 7.5425\n",
      "  Batch 9000/17006, Loss: 7.0783\n",
      "  Batch 9100/17006, Loss: 6.7697\n",
      "  Batch 9200/17006, Loss: 7.3372\n",
      "  Batch 9300/17006, Loss: 7.7203\n",
      "  Batch 9400/17006, Loss: 7.1349\n",
      "  Batch 9500/17006, Loss: 6.2589\n",
      "  Batch 9600/17006, Loss: 7.0571\n",
      "  Batch 9700/17006, Loss: 6.8777\n",
      "  Batch 9800/17006, Loss: 6.9017\n",
      "  Batch 9900/17006, Loss: 7.4367\n",
      "  Batch 10000/17006, Loss: 7.1117\n",
      "  Batch 10100/17006, Loss: 7.3798\n",
      "  Batch 10200/17006, Loss: 6.9649\n",
      "  Batch 10300/17006, Loss: 6.8658\n",
      "  Batch 10400/17006, Loss: 7.3243\n",
      "  Batch 10500/17006, Loss: 6.9122\n",
      "  Batch 10600/17006, Loss: 6.7337\n",
      "  Batch 10700/17006, Loss: 7.2525\n",
      "  Batch 10800/17006, Loss: 7.3805\n",
      "  Batch 10900/17006, Loss: 6.6356\n",
      "  Batch 11000/17006, Loss: 7.3885\n",
      "  Batch 11100/17006, Loss: 6.8552\n",
      "  Batch 11200/17006, Loss: 6.5720\n",
      "  Batch 11300/17006, Loss: 6.7624\n",
      "  Batch 11400/17006, Loss: 6.8949\n",
      "  Batch 11500/17006, Loss: 6.7877\n",
      "  Batch 11600/17006, Loss: 6.8740\n",
      "  Batch 11700/17006, Loss: 6.5744\n",
      "  Batch 11800/17006, Loss: 7.0841\n",
      "  Batch 11900/17006, Loss: 7.1674\n",
      "  Batch 12000/17006, Loss: 7.1639\n",
      "  Batch 12100/17006, Loss: 6.9584\n",
      "  Batch 12200/17006, Loss: 7.3776\n",
      "  Batch 12300/17006, Loss: 6.9511\n",
      "  Batch 12400/17006, Loss: 6.9268\n",
      "  Batch 12500/17006, Loss: 7.0484\n",
      "  Batch 12600/17006, Loss: 7.1966\n",
      "  Batch 12700/17006, Loss: 7.4394\n",
      "  Batch 12800/17006, Loss: 7.2738\n",
      "  Batch 12900/17006, Loss: 7.2975\n",
      "  Batch 13000/17006, Loss: 7.0406\n",
      "  Batch 13100/17006, Loss: 6.6756\n",
      "  Batch 13200/17006, Loss: 7.2317\n",
      "  Batch 13300/17006, Loss: 7.1190\n",
      "  Batch 13400/17006, Loss: 6.7760\n",
      "  Batch 13500/17006, Loss: 6.6738\n",
      "  Batch 13600/17006, Loss: 6.9204\n",
      "  Batch 13700/17006, Loss: 6.8946\n",
      "  Batch 13800/17006, Loss: 7.1886\n",
      "  Batch 13900/17006, Loss: 7.5371\n",
      "  Batch 14000/17006, Loss: 7.0215\n",
      "  Batch 14100/17006, Loss: 7.2718\n",
      "  Batch 14200/17006, Loss: 6.8534\n",
      "  Batch 14300/17006, Loss: 6.7772\n",
      "  Batch 14400/17006, Loss: 6.6514\n",
      "  Batch 14500/17006, Loss: 6.8077\n",
      "  Batch 14600/17006, Loss: 7.0657\n",
      "  Batch 14700/17006, Loss: 7.0929\n",
      "  Batch 14800/17006, Loss: 7.2067\n",
      "  Batch 14900/17006, Loss: 7.4874\n",
      "  Batch 15000/17006, Loss: 6.9469\n",
      "  Batch 15100/17006, Loss: 7.2407\n",
      "  Batch 15200/17006, Loss: 6.7618\n",
      "  Batch 15300/17006, Loss: 7.1717\n",
      "  Batch 15400/17006, Loss: 6.5090\n",
      "  Batch 15500/17006, Loss: 6.9997\n",
      "  Batch 15600/17006, Loss: 6.4279\n",
      "  Batch 15700/17006, Loss: 6.9851\n",
      "  Batch 15800/17006, Loss: 6.8851\n",
      "  Batch 15900/17006, Loss: 6.6177\n",
      "  Batch 16000/17006, Loss: 6.8743\n",
      "  Batch 16100/17006, Loss: 6.7766\n",
      "  Batch 16200/17006, Loss: 6.8246\n",
      "  Batch 16300/17006, Loss: 7.3212\n",
      "  Batch 16400/17006, Loss: 6.8837\n",
      "  Batch 16500/17006, Loss: 6.7924\n",
      "  Batch 16600/17006, Loss: 7.2089\n",
      "  Batch 16700/17006, Loss: 7.0866\n",
      "  Batch 16800/17006, Loss: 7.1623\n",
      "  Batch 16900/17006, Loss: 6.6669\n",
      "  Batch 17000/17006, Loss: 7.0434\n",
      "Epoch 2 finished. Average Loss: 7.0878\n",
      "\n",
      "--- Epoch 3/5 ---\n",
      "  Batch 100/17006, Loss: 7.0580\n",
      "  Batch 200/17006, Loss: 7.1941\n",
      "  Batch 300/17006, Loss: 7.1736\n",
      "  Batch 400/17006, Loss: 7.0723\n",
      "  Batch 500/17006, Loss: 6.9045\n",
      "  Batch 600/17006, Loss: 6.9632\n",
      "  Batch 700/17006, Loss: 6.9011\n",
      "  Batch 800/17006, Loss: 6.4724\n",
      "  Batch 900/17006, Loss: 6.4829\n",
      "  Batch 1000/17006, Loss: 6.9555\n",
      "  Batch 1100/17006, Loss: 7.1015\n",
      "  Batch 1200/17006, Loss: 6.0563\n",
      "  Batch 1300/17006, Loss: 6.6726\n",
      "  Batch 1400/17006, Loss: 6.4507\n",
      "  Batch 1500/17006, Loss: 6.8687\n",
      "  Batch 1600/17006, Loss: 6.8394\n",
      "  Batch 1700/17006, Loss: 7.1170\n",
      "  Batch 1800/17006, Loss: 6.9748\n",
      "  Batch 1900/17006, Loss: 7.0440\n",
      "  Batch 2000/17006, Loss: 6.7143\n",
      "  Batch 2100/17006, Loss: 6.3116\n",
      "  Batch 2200/17006, Loss: 6.6746\n",
      "  Batch 2300/17006, Loss: 6.7151\n",
      "  Batch 2400/17006, Loss: 7.1074\n",
      "  Batch 2500/17006, Loss: 6.8639\n",
      "  Batch 2600/17006, Loss: 6.7144\n",
      "  Batch 2700/17006, Loss: 6.4331\n",
      "  Batch 2800/17006, Loss: 6.8960\n",
      "  Batch 2900/17006, Loss: 7.1037\n",
      "  Batch 3000/17006, Loss: 7.1125\n",
      "  Batch 3100/17006, Loss: 7.0038\n",
      "  Batch 3200/17006, Loss: 6.8031\n",
      "  Batch 3300/17006, Loss: 6.4383\n",
      "  Batch 3400/17006, Loss: 6.4324\n",
      "  Batch 3500/17006, Loss: 6.9372\n",
      "  Batch 3600/17006, Loss: 7.0022\n",
      "  Batch 3700/17006, Loss: 6.9822\n",
      "  Batch 3800/17006, Loss: 6.9231\n",
      "  Batch 3900/17006, Loss: 7.2965\n",
      "  Batch 4000/17006, Loss: 6.3037\n",
      "  Batch 4100/17006, Loss: 7.1983\n",
      "  Batch 4200/17006, Loss: 6.8878\n",
      "  Batch 4300/17006, Loss: 7.0911\n",
      "  Batch 4400/17006, Loss: 6.6063\n",
      "  Batch 4500/17006, Loss: 6.9908\n",
      "  Batch 4600/17006, Loss: 6.5530\n",
      "  Batch 4700/17006, Loss: 6.6879\n",
      "  Batch 4800/17006, Loss: 6.5286\n",
      "  Batch 4900/17006, Loss: 6.9486\n",
      "  Batch 5000/17006, Loss: 6.9255\n",
      "  Batch 5100/17006, Loss: 7.0547\n",
      "  Batch 5200/17006, Loss: 7.1093\n",
      "  Batch 5300/17006, Loss: 6.9865\n",
      "  Batch 5400/17006, Loss: 6.9201\n",
      "  Batch 5500/17006, Loss: 6.4832\n",
      "  Batch 5600/17006, Loss: 6.7708\n",
      "  Batch 5700/17006, Loss: 7.1582\n",
      "  Batch 5800/17006, Loss: 7.2895\n",
      "  Batch 5900/17006, Loss: 7.0756\n",
      "  Batch 6000/17006, Loss: 6.8828\n",
      "  Batch 6100/17006, Loss: 6.1572\n",
      "  Batch 6200/17006, Loss: 6.9513\n",
      "  Batch 6300/17006, Loss: 6.4095\n",
      "  Batch 6400/17006, Loss: 6.7113\n",
      "  Batch 6500/17006, Loss: 5.9078\n",
      "  Batch 6600/17006, Loss: 6.9343\n",
      "  Batch 6700/17006, Loss: 6.8608\n",
      "  Batch 6800/17006, Loss: 6.9061\n",
      "  Batch 6900/17006, Loss: 6.6704\n",
      "  Batch 7000/17006, Loss: 6.9462\n",
      "  Batch 7100/17006, Loss: 6.5925\n",
      "  Batch 7200/17006, Loss: 6.7857\n",
      "  Batch 7300/17006, Loss: 6.3318\n",
      "  Batch 7400/17006, Loss: 6.7301\n",
      "  Batch 7500/17006, Loss: 6.8480\n",
      "  Batch 7600/17006, Loss: 6.8912\n",
      "  Batch 7700/17006, Loss: 6.5003\n",
      "  Batch 7800/17006, Loss: 6.4574\n",
      "  Batch 7900/17006, Loss: 7.0223\n",
      "  Batch 8000/17006, Loss: 6.6656\n",
      "  Batch 8100/17006, Loss: 7.0482\n",
      "  Batch 8200/17006, Loss: 6.5650\n",
      "  Batch 8300/17006, Loss: 6.3360\n",
      "  Batch 8400/17006, Loss: 6.4661\n",
      "  Batch 8500/17006, Loss: 7.2060\n",
      "  Batch 8600/17006, Loss: 6.3913\n",
      "  Batch 8700/17006, Loss: 6.9122\n",
      "  Batch 8800/17006, Loss: 6.5950\n",
      "  Batch 8900/17006, Loss: 6.8928\n",
      "  Batch 9000/17006, Loss: 6.5695\n",
      "  Batch 9100/17006, Loss: 7.7001\n",
      "  Batch 9200/17006, Loss: 6.4703\n",
      "  Batch 9300/17006, Loss: 6.7257\n",
      "  Batch 9400/17006, Loss: 6.7927\n",
      "  Batch 9500/17006, Loss: 6.1299\n",
      "  Batch 9600/17006, Loss: 6.1872\n",
      "  Batch 9700/17006, Loss: 6.2189\n",
      "  Batch 9800/17006, Loss: 7.0334\n",
      "  Batch 9900/17006, Loss: 6.5818\n",
      "  Batch 10000/17006, Loss: 6.4996\n",
      "  Batch 10100/17006, Loss: 6.2563\n",
      "  Batch 10200/17006, Loss: 6.6518\n",
      "  Batch 10300/17006, Loss: 6.3231\n",
      "  Batch 10400/17006, Loss: 6.8696\n",
      "  Batch 10500/17006, Loss: 6.8212\n",
      "  Batch 10600/17006, Loss: 6.9473\n",
      "  Batch 10700/17006, Loss: 6.7362\n",
      "  Batch 10800/17006, Loss: 6.6067\n",
      "  Batch 10900/17006, Loss: 6.8341\n",
      "  Batch 11000/17006, Loss: 6.1184\n",
      "  Batch 11100/17006, Loss: 6.4810\n",
      "  Batch 11200/17006, Loss: 6.7360\n",
      "  Batch 11300/17006, Loss: 7.0038\n",
      "  Batch 11400/17006, Loss: 6.4779\n",
      "  Batch 11500/17006, Loss: 6.7159\n",
      "  Batch 11600/17006, Loss: 7.3293\n",
      "  Batch 11700/17006, Loss: 6.8167\n",
      "  Batch 11800/17006, Loss: 7.1323\n",
      "  Batch 11900/17006, Loss: 6.4747\n",
      "  Batch 12000/17006, Loss: 6.5304\n",
      "  Batch 12100/17006, Loss: 7.2419\n",
      "  Batch 12200/17006, Loss: 6.4825\n",
      "  Batch 12300/17006, Loss: 7.3413\n",
      "  Batch 12400/17006, Loss: 6.9467\n",
      "  Batch 12500/17006, Loss: 6.5687\n",
      "  Batch 12600/17006, Loss: 6.7874\n",
      "  Batch 12700/17006, Loss: 6.5559\n",
      "  Batch 12800/17006, Loss: 6.8852\n",
      "  Batch 12900/17006, Loss: 6.3500\n",
      "  Batch 13000/17006, Loss: 6.6767\n",
      "  Batch 13100/17006, Loss: 6.3496\n",
      "  Batch 13200/17006, Loss: 6.5235\n",
      "  Batch 13300/17006, Loss: 6.5162\n",
      "  Batch 13400/17006, Loss: 6.3203\n",
      "  Batch 13500/17006, Loss: 6.9725\n",
      "  Batch 13600/17006, Loss: 7.1432\n",
      "  Batch 13700/17006, Loss: 6.7579\n",
      "  Batch 13800/17006, Loss: 6.6889\n",
      "  Batch 13900/17006, Loss: 6.8625\n",
      "  Batch 14000/17006, Loss: 5.8617\n",
      "  Batch 14100/17006, Loss: 6.2612\n",
      "  Batch 14200/17006, Loss: 7.1654\n",
      "  Batch 14300/17006, Loss: 6.7510\n",
      "  Batch 14400/17006, Loss: 6.7281\n",
      "  Batch 14500/17006, Loss: 6.5265\n",
      "  Batch 14600/17006, Loss: 6.3784\n",
      "  Batch 14700/17006, Loss: 6.5754\n",
      "  Batch 14800/17006, Loss: 6.4885\n",
      "  Batch 14900/17006, Loss: 6.1432\n",
      "  Batch 15000/17006, Loss: 6.6332\n",
      "  Batch 15100/17006, Loss: 5.9440\n",
      "  Batch 15200/17006, Loss: 6.2874\n",
      "  Batch 15300/17006, Loss: 6.6218\n",
      "  Batch 15400/17006, Loss: 6.3000\n",
      "  Batch 15500/17006, Loss: 6.6287\n",
      "  Batch 15600/17006, Loss: 6.2199\n",
      "  Batch 15700/17006, Loss: 6.2362\n",
      "  Batch 15800/17006, Loss: 6.8114\n",
      "  Batch 15900/17006, Loss: 6.2159\n",
      "  Batch 16000/17006, Loss: 6.4289\n",
      "  Batch 16100/17006, Loss: 6.5589\n",
      "  Batch 16200/17006, Loss: 6.6085\n",
      "  Batch 16300/17006, Loss: 6.1593\n",
      "  Batch 16400/17006, Loss: 6.9690\n",
      "  Batch 16500/17006, Loss: 6.2317\n",
      "  Batch 16600/17006, Loss: 6.4783\n",
      "  Batch 16700/17006, Loss: 5.7932\n",
      "  Batch 16800/17006, Loss: 6.2359\n",
      "  Batch 16900/17006, Loss: 6.4487\n",
      "  Batch 17000/17006, Loss: 6.7230\n",
      "Epoch 3 finished. Average Loss: 6.6978\n",
      "\n",
      "--- Epoch 4/5 ---\n",
      "  Batch 100/17006, Loss: 6.2781\n",
      "  Batch 200/17006, Loss: 6.4922\n",
      "  Batch 300/17006, Loss: 6.2887\n",
      "  Batch 400/17006, Loss: 6.1686\n",
      "  Batch 500/17006, Loss: 6.6414\n",
      "  Batch 600/17006, Loss: 6.3743\n",
      "  Batch 700/17006, Loss: 6.0490\n",
      "  Batch 800/17006, Loss: 6.4619\n",
      "  Batch 900/17006, Loss: 6.8452\n",
      "  Batch 1000/17006, Loss: 5.9889\n",
      "  Batch 1100/17006, Loss: 6.5508\n",
      "  Batch 1200/17006, Loss: 6.7921\n",
      "  Batch 1300/17006, Loss: 6.3295\n",
      "  Batch 1400/17006, Loss: 6.3415\n",
      "  Batch 1500/17006, Loss: 6.5420\n",
      "  Batch 1600/17006, Loss: 6.3783\n",
      "  Batch 1700/17006, Loss: 7.0427\n",
      "  Batch 1800/17006, Loss: 6.3559\n",
      "  Batch 1900/17006, Loss: 6.5051\n",
      "  Batch 2000/17006, Loss: 6.4311\n",
      "  Batch 2100/17006, Loss: 6.3824\n",
      "  Batch 2200/17006, Loss: 6.2601\n",
      "  Batch 2300/17006, Loss: 6.9357\n",
      "  Batch 2400/17006, Loss: 6.5268\n",
      "  Batch 2500/17006, Loss: 6.0898\n",
      "  Batch 2600/17006, Loss: 6.6981\n",
      "  Batch 2700/17006, Loss: 6.2760\n",
      "  Batch 2800/17006, Loss: 6.0275\n",
      "  Batch 2900/17006, Loss: 6.2860\n",
      "  Batch 3000/17006, Loss: 6.6241\n",
      "  Batch 3100/17006, Loss: 6.3736\n",
      "  Batch 3200/17006, Loss: 5.9757\n",
      "  Batch 3300/17006, Loss: 6.2158\n",
      "  Batch 3400/17006, Loss: 6.3911\n",
      "  Batch 3500/17006, Loss: 6.7967\n",
      "  Batch 3600/17006, Loss: 6.2946\n",
      "  Batch 3700/17006, Loss: 6.5217\n",
      "  Batch 3800/17006, Loss: 6.5943\n",
      "  Batch 3900/17006, Loss: 6.5432\n",
      "  Batch 4000/17006, Loss: 6.1394\n",
      "  Batch 4100/17006, Loss: 6.2724\n",
      "  Batch 4200/17006, Loss: 6.7372\n",
      "  Batch 4300/17006, Loss: 6.3406\n",
      "  Batch 4400/17006, Loss: 6.3079\n",
      "  Batch 4500/17006, Loss: 6.3682\n",
      "  Batch 4600/17006, Loss: 6.6282\n",
      "  Batch 4700/17006, Loss: 6.6357\n",
      "  Batch 4800/17006, Loss: 6.4453\n",
      "  Batch 4900/17006, Loss: 6.0615\n",
      "  Batch 5000/17006, Loss: 6.0597\n",
      "  Batch 5100/17006, Loss: 6.5159\n",
      "  Batch 5200/17006, Loss: 6.2726\n",
      "  Batch 5300/17006, Loss: 6.5973\n",
      "  Batch 5400/17006, Loss: 6.3293\n",
      "  Batch 5500/17006, Loss: 6.6820\n",
      "  Batch 5600/17006, Loss: 6.5827\n",
      "  Batch 5700/17006, Loss: 6.3791\n",
      "  Batch 5800/17006, Loss: 6.2803\n",
      "  Batch 5900/17006, Loss: 6.7296\n",
      "  Batch 6000/17006, Loss: 6.6062\n",
      "  Batch 6100/17006, Loss: 6.6513\n",
      "  Batch 6200/17006, Loss: 6.7869\n",
      "  Batch 6300/17006, Loss: 6.3520\n",
      "  Batch 6400/17006, Loss: 6.0775\n",
      "  Batch 6500/17006, Loss: 6.2678\n",
      "  Batch 6600/17006, Loss: 6.0968\n",
      "  Batch 6700/17006, Loss: 6.6280\n",
      "  Batch 6800/17006, Loss: 6.3799\n",
      "  Batch 6900/17006, Loss: 5.9958\n",
      "  Batch 7000/17006, Loss: 6.3443\n",
      "  Batch 7100/17006, Loss: 6.2818\n",
      "  Batch 7200/17006, Loss: 6.6350\n",
      "  Batch 7300/17006, Loss: 6.3116\n",
      "  Batch 7400/17006, Loss: 6.4010\n",
      "  Batch 7500/17006, Loss: 6.4850\n",
      "  Batch 7600/17006, Loss: 5.7567\n",
      "  Batch 7700/17006, Loss: 6.2132\n",
      "  Batch 7800/17006, Loss: 6.2892\n",
      "  Batch 7900/17006, Loss: 6.8454\n",
      "  Batch 8000/17006, Loss: 5.8071\n",
      "  Batch 8100/17006, Loss: 6.3655\n",
      "  Batch 8200/17006, Loss: 6.1292\n",
      "  Batch 8300/17006, Loss: 6.3217\n",
      "  Batch 8400/17006, Loss: 5.9656\n",
      "  Batch 8500/17006, Loss: 6.6316\n",
      "  Batch 8600/17006, Loss: 6.1088\n",
      "  Batch 8700/17006, Loss: 6.4902\n",
      "  Batch 8800/17006, Loss: 6.0545\n",
      "  Batch 8900/17006, Loss: 6.1903\n",
      "  Batch 9000/17006, Loss: 6.2143\n",
      "  Batch 9100/17006, Loss: 6.6730\n",
      "  Batch 9200/17006, Loss: 6.1832\n",
      "  Batch 9300/17006, Loss: 5.6082\n",
      "  Batch 9400/17006, Loss: 5.8430\n",
      "  Batch 9500/17006, Loss: 5.6421\n",
      "  Batch 9600/17006, Loss: 5.9902\n",
      "  Batch 9700/17006, Loss: 6.5385\n",
      "  Batch 9800/17006, Loss: 6.5197\n",
      "  Batch 9900/17006, Loss: 6.4067\n",
      "  Batch 10000/17006, Loss: 6.0635\n",
      "  Batch 10100/17006, Loss: 6.3125\n",
      "  Batch 10200/17006, Loss: 5.8229\n",
      "  Batch 10300/17006, Loss: 6.2715\n",
      "  Batch 10400/17006, Loss: 6.2840\n",
      "  Batch 10500/17006, Loss: 6.6317\n",
      "  Batch 10600/17006, Loss: 6.3701\n",
      "  Batch 10700/17006, Loss: 6.2279\n",
      "  Batch 10800/17006, Loss: 6.3288\n",
      "  Batch 10900/17006, Loss: 5.6590\n",
      "  Batch 11000/17006, Loss: 5.9743\n",
      "  Batch 11100/17006, Loss: 6.6439\n",
      "  Batch 11200/17006, Loss: 6.1888\n",
      "  Batch 11300/17006, Loss: 6.6212\n",
      "  Batch 11400/17006, Loss: 6.0246\n",
      "  Batch 11500/17006, Loss: 6.4983\n",
      "  Batch 11600/17006, Loss: 6.1234\n",
      "  Batch 11700/17006, Loss: 6.1399\n",
      "  Batch 11800/17006, Loss: 6.4570\n",
      "  Batch 11900/17006, Loss: 5.9968\n",
      "  Batch 12000/17006, Loss: 6.4139\n",
      "  Batch 12100/17006, Loss: 6.6388\n",
      "  Batch 12200/17006, Loss: 6.5540\n",
      "  Batch 12300/17006, Loss: 5.9357\n",
      "  Batch 12400/17006, Loss: 5.7538\n",
      "  Batch 12500/17006, Loss: 5.7746\n",
      "  Batch 12600/17006, Loss: 5.7932\n",
      "  Batch 12700/17006, Loss: 6.1812\n",
      "  Batch 12800/17006, Loss: 6.6671\n",
      "  Batch 12900/17006, Loss: 6.1923\n",
      "  Batch 13000/17006, Loss: 6.2870\n",
      "  Batch 13100/17006, Loss: 6.0918\n",
      "  Batch 13200/17006, Loss: 6.8873\n",
      "  Batch 13300/17006, Loss: 6.1285\n",
      "  Batch 13400/17006, Loss: 6.4295\n",
      "  Batch 13500/17006, Loss: 6.0518\n",
      "  Batch 13600/17006, Loss: 6.1841\n",
      "  Batch 13700/17006, Loss: 5.8990\n",
      "  Batch 13800/17006, Loss: 6.4860\n",
      "  Batch 13900/17006, Loss: 6.1934\n",
      "  Batch 14000/17006, Loss: 5.9525\n",
      "  Batch 14100/17006, Loss: 6.2351\n",
      "  Batch 14200/17006, Loss: 5.7664\n",
      "  Batch 14300/17006, Loss: 6.1081\n",
      "  Batch 14400/17006, Loss: 5.7909\n",
      "  Batch 14500/17006, Loss: 6.0159\n",
      "  Batch 14600/17006, Loss: 5.8056\n",
      "  Batch 14700/17006, Loss: 6.1332\n",
      "  Batch 14800/17006, Loss: 6.3890\n",
      "  Batch 14900/17006, Loss: 6.4325\n",
      "  Batch 15000/17006, Loss: 6.2794\n",
      "  Batch 15100/17006, Loss: 6.2867\n",
      "  Batch 15200/17006, Loss: 6.2455\n",
      "  Batch 15300/17006, Loss: 6.4268\n",
      "  Batch 15400/17006, Loss: 6.5608\n",
      "  Batch 15500/17006, Loss: 6.4956\n",
      "  Batch 15600/17006, Loss: 6.1615\n",
      "  Batch 15700/17006, Loss: 6.1625\n",
      "  Batch 15800/17006, Loss: 6.0587\n",
      "  Batch 15900/17006, Loss: 6.2572\n",
      "  Batch 16000/17006, Loss: 6.3537\n",
      "  Batch 16100/17006, Loss: 6.2350\n",
      "  Batch 16200/17006, Loss: 6.2533\n",
      "  Batch 16300/17006, Loss: 6.3902\n",
      "  Batch 16400/17006, Loss: 6.6928\n",
      "  Batch 16500/17006, Loss: 6.0420\n",
      "  Batch 16600/17006, Loss: 5.8402\n",
      "  Batch 16700/17006, Loss: 6.3749\n",
      "  Batch 16800/17006, Loss: 5.8828\n",
      "  Batch 16900/17006, Loss: 5.7568\n",
      "  Batch 17000/17006, Loss: 6.2967\n",
      "Epoch 4 finished. Average Loss: 6.3180\n",
      "\n",
      "--- Epoch 5/5 ---\n",
      "  Batch 100/17006, Loss: 6.4404\n",
      "  Batch 200/17006, Loss: 6.1427\n",
      "  Batch 300/17006, Loss: 6.0209\n",
      "  Batch 400/17006, Loss: 5.5546\n",
      "  Batch 500/17006, Loss: 6.1880\n",
      "  Batch 600/17006, Loss: 6.3208\n",
      "  Batch 700/17006, Loss: 6.3013\n",
      "  Batch 800/17006, Loss: 6.6504\n",
      "  Batch 900/17006, Loss: 6.1785\n",
      "  Batch 1000/17006, Loss: 6.1250\n",
      "  Batch 1100/17006, Loss: 6.0146\n",
      "  Batch 1200/17006, Loss: 5.7996\n",
      "  Batch 1300/17006, Loss: 5.6884\n",
      "  Batch 1400/17006, Loss: 5.9865\n",
      "  Batch 1500/17006, Loss: 6.8013\n",
      "  Batch 1600/17006, Loss: 5.5475\n",
      "  Batch 1700/17006, Loss: 6.1566\n",
      "  Batch 1800/17006, Loss: 6.0776\n",
      "  Batch 1900/17006, Loss: 6.0956\n",
      "  Batch 2000/17006, Loss: 6.0614\n",
      "  Batch 2100/17006, Loss: 5.6571\n",
      "  Batch 2200/17006, Loss: 6.1188\n",
      "  Batch 2300/17006, Loss: 6.1092\n",
      "  Batch 2400/17006, Loss: 6.2845\n",
      "  Batch 2500/17006, Loss: 6.0051\n",
      "  Batch 2600/17006, Loss: 6.6526\n",
      "  Batch 2700/17006, Loss: 6.1554\n",
      "  Batch 2800/17006, Loss: 6.5755\n",
      "  Batch 2900/17006, Loss: 6.2048\n",
      "  Batch 3000/17006, Loss: 6.0795\n",
      "  Batch 3100/17006, Loss: 6.3281\n",
      "  Batch 3200/17006, Loss: 5.6953\n",
      "  Batch 3300/17006, Loss: 5.9006\n",
      "  Batch 3400/17006, Loss: 5.9345\n",
      "  Batch 3500/17006, Loss: 6.5686\n",
      "  Batch 3600/17006, Loss: 6.5204\n",
      "  Batch 3700/17006, Loss: 5.9541\n",
      "  Batch 3800/17006, Loss: 5.8828\n",
      "  Batch 3900/17006, Loss: 5.9053\n",
      "  Batch 4000/17006, Loss: 6.3244\n",
      "  Batch 4100/17006, Loss: 6.2598\n",
      "  Batch 4200/17006, Loss: 5.9157\n",
      "  Batch 4300/17006, Loss: 6.1151\n",
      "  Batch 4400/17006, Loss: 6.3341\n",
      "  Batch 4500/17006, Loss: 5.8926\n",
      "  Batch 4600/17006, Loss: 6.2521\n",
      "  Batch 4700/17006, Loss: 6.0840\n",
      "  Batch 4800/17006, Loss: 5.9275\n",
      "  Batch 4900/17006, Loss: 5.6832\n",
      "  Batch 5000/17006, Loss: 6.5851\n",
      "  Batch 5100/17006, Loss: 5.8160\n",
      "  Batch 5200/17006, Loss: 6.5234\n",
      "  Batch 5300/17006, Loss: 6.7166\n",
      "  Batch 5400/17006, Loss: 5.9954\n",
      "  Batch 5500/17006, Loss: 6.6557\n",
      "  Batch 5600/17006, Loss: 5.9059\n",
      "  Batch 5700/17006, Loss: 6.1401\n",
      "  Batch 5800/17006, Loss: 5.6943\n",
      "  Batch 5900/17006, Loss: 6.5164\n",
      "  Batch 6000/17006, Loss: 6.0703\n",
      "  Batch 6100/17006, Loss: 6.7601\n",
      "  Batch 6200/17006, Loss: 6.6043\n",
      "  Batch 6300/17006, Loss: 6.0565\n",
      "  Batch 6400/17006, Loss: 5.9271\n",
      "  Batch 6500/17006, Loss: 6.2572\n",
      "  Batch 6600/17006, Loss: 6.1607\n",
      "  Batch 6700/17006, Loss: 6.0918\n",
      "  Batch 6800/17006, Loss: 6.1527\n",
      "  Batch 6900/17006, Loss: 5.4212\n",
      "  Batch 7000/17006, Loss: 5.9157\n",
      "  Batch 7100/17006, Loss: 6.6503\n",
      "  Batch 7200/17006, Loss: 5.7829\n",
      "  Batch 7300/17006, Loss: 6.2924\n",
      "  Batch 7400/17006, Loss: 6.1622\n",
      "  Batch 7500/17006, Loss: 5.8723\n",
      "  Batch 7600/17006, Loss: 5.7584\n",
      "  Batch 7700/17006, Loss: 6.3306\n",
      "  Batch 7800/17006, Loss: 5.9235\n",
      "  Batch 7900/17006, Loss: 6.2217\n",
      "  Batch 8000/17006, Loss: 5.9470\n",
      "  Batch 8100/17006, Loss: 6.1685\n",
      "  Batch 8200/17006, Loss: 5.7965\n",
      "  Batch 8300/17006, Loss: 5.9816\n",
      "  Batch 8400/17006, Loss: 5.2128\n",
      "  Batch 8500/17006, Loss: 6.2352\n",
      "  Batch 8600/17006, Loss: 6.0624\n",
      "  Batch 8700/17006, Loss: 5.9848\n",
      "  Batch 8800/17006, Loss: 5.4007\n",
      "  Batch 8900/17006, Loss: 5.8802\n",
      "  Batch 9000/17006, Loss: 5.9332\n",
      "  Batch 9100/17006, Loss: 5.8234\n",
      "  Batch 9200/17006, Loss: 6.0859\n",
      "  Batch 9300/17006, Loss: 6.3393\n",
      "  Batch 9400/17006, Loss: 6.2473\n",
      "  Batch 9500/17006, Loss: 6.1571\n",
      "  Batch 9600/17006, Loss: 5.9436\n",
      "  Batch 9700/17006, Loss: 5.8754\n",
      "  Batch 9800/17006, Loss: 5.8014\n",
      "  Batch 9900/17006, Loss: 6.4964\n",
      "  Batch 10000/17006, Loss: 5.9864\n",
      "  Batch 10100/17006, Loss: 5.5626\n",
      "  Batch 10200/17006, Loss: 6.6289\n",
      "  Batch 10300/17006, Loss: 6.2550\n",
      "  Batch 10400/17006, Loss: 5.4566\n",
      "  Batch 10500/17006, Loss: 6.1242\n",
      "  Batch 10600/17006, Loss: 6.1668\n",
      "  Batch 10700/17006, Loss: 6.0264\n",
      "  Batch 10800/17006, Loss: 6.1777\n",
      "  Batch 10900/17006, Loss: 5.8792\n",
      "  Batch 11000/17006, Loss: 5.8577\n",
      "  Batch 11100/17006, Loss: 6.6003\n",
      "  Batch 11200/17006, Loss: 5.8775\n",
      "  Batch 11300/17006, Loss: 5.8797\n",
      "  Batch 11400/17006, Loss: 5.9981\n",
      "  Batch 11500/17006, Loss: 5.9083\n",
      "  Batch 11600/17006, Loss: 6.4826\n",
      "  Batch 11700/17006, Loss: 5.7840\n",
      "  Batch 11800/17006, Loss: 6.2160\n",
      "  Batch 11900/17006, Loss: 6.4095\n",
      "  Batch 12000/17006, Loss: 5.7744\n",
      "  Batch 12100/17006, Loss: 5.6224\n",
      "  Batch 12200/17006, Loss: 5.8396\n",
      "  Batch 12300/17006, Loss: 6.4340\n",
      "  Batch 12400/17006, Loss: 5.5450\n",
      "  Batch 12500/17006, Loss: 5.8267\n",
      "  Batch 12600/17006, Loss: 6.0527\n",
      "  Batch 12700/17006, Loss: 6.2790\n",
      "  Batch 12800/17006, Loss: 6.1083\n",
      "  Batch 12900/17006, Loss: 6.1818\n",
      "  Batch 13000/17006, Loss: 5.8831\n",
      "  Batch 13100/17006, Loss: 5.6543\n",
      "  Batch 13200/17006, Loss: 6.1057\n",
      "  Batch 13300/17006, Loss: 5.7421\n",
      "  Batch 13400/17006, Loss: 5.6046\n",
      "  Batch 13500/17006, Loss: 6.1776\n",
      "  Batch 13600/17006, Loss: 5.4982\n",
      "  Batch 13700/17006, Loss: 6.0826\n",
      "  Batch 13800/17006, Loss: 6.2288\n",
      "  Batch 13900/17006, Loss: 6.1039\n",
      "  Batch 14000/17006, Loss: 6.1549\n",
      "  Batch 14100/17006, Loss: 5.3127\n",
      "  Batch 14200/17006, Loss: 6.0572\n",
      "  Batch 14300/17006, Loss: 6.0803\n",
      "  Batch 14400/17006, Loss: 5.8880\n",
      "  Batch 14500/17006, Loss: 5.7157\n",
      "  Batch 14600/17006, Loss: 5.3061\n",
      "  Batch 14700/17006, Loss: 5.3413\n",
      "  Batch 14800/17006, Loss: 5.7514\n",
      "  Batch 14900/17006, Loss: 6.2566\n",
      "  Batch 15000/17006, Loss: 5.6878\n",
      "  Batch 15100/17006, Loss: 5.7540\n",
      "  Batch 15200/17006, Loss: 5.8878\n",
      "  Batch 15300/17006, Loss: 6.1845\n",
      "  Batch 15400/17006, Loss: 5.4503\n",
      "  Batch 15500/17006, Loss: 5.7844\n",
      "  Batch 15600/17006, Loss: 5.5570\n",
      "  Batch 15700/17006, Loss: 5.5620\n",
      "  Batch 15800/17006, Loss: 6.0592\n",
      "  Batch 15900/17006, Loss: 5.9367\n",
      "  Batch 16000/17006, Loss: 5.4808\n",
      "  Batch 16100/17006, Loss: 6.0791\n",
      "  Batch 16200/17006, Loss: 5.9092\n",
      "  Batch 16300/17006, Loss: 6.5016\n",
      "  Batch 16400/17006, Loss: 6.0466\n",
      "  Batch 16500/17006, Loss: 6.1558\n",
      "  Batch 16600/17006, Loss: 5.5942\n",
      "  Batch 16700/17006, Loss: 5.9726\n",
      "  Batch 16800/17006, Loss: 4.8708\n",
      "  Batch 16900/17006, Loss: 5.9016\n",
      "  Batch 17000/17006, Loss: 5.9015\n",
      "Epoch 5 finished. Average Loss: 6.0111\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# --- Assume previous code for model, dataset, etc. is loaded ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"wiki-bpe-tokenizer.json\")\n",
    "\n",
    "# Initialize the Dataset and DataLoader\n",
    "train_dataset = MLMDataset(\"wiki_corpus.txt\", tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the Model\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "EMBED_SIZE = 256\n",
    "NUM_LAYERS = 6\n",
    "HEADS = 8\n",
    "FORWARD_EXPANSION = 4\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, EMBED_SIZE, NUM_LAYERS, HEADS, device, FORWARD_EXPANSION, DROPOUT, MAX_LENGTH)\n",
    "model = BERT(encoder, VOCAB_SIZE).to(device)\n",
    "\n",
    "# Initialize Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# --- The Training Loop ---\n",
    "epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, mask=attention_mask)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        # The outputs tensor is reshaped to (batch_size * seq_len, vocab_size)\n",
    "        outputs = outputs.view(-1, VOCAB_SIZE)\n",
    "\n",
    "        # FIX: The labels tensor should be flattened to (batch_size * seq_len)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_mlm_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'optimizer' are your trained model and optimizer objects\n",
    "# and 'epoch' is the last completed epoch number.\n",
    "\n",
    "# Define a path to save the checkpoint\n",
    "SAVE_PATH = \"bert_mlm_checkpoint.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss, # You can save the last loss value too\n",
    "    }, SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERT:\n\tMissing key(s) in state_dict: \"encoder.word_embedding.weight\", \"encoder.position_embedding.weight\", \"encoder.layers.0.attention.values.weight\", \"encoder.layers.0.attention.keys.weight\", \"encoder.layers.0.attention.query.weight\", \"encoder.layers.0.attention.fc_out.weight\", \"encoder.layers.0.attention.fc_out.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.0.feed_forward.0.weight\", \"encoder.layers.0.feed_forward.0.bias\", \"encoder.layers.0.feed_forward.2.weight\", \"encoder.layers.0.feed_forward.2.bias\", \"encoder.layers.1.attention.values.weight\", \"encoder.layers.1.attention.keys.weight\", \"encoder.layers.1.attention.query.weight\", \"encoder.layers.1.attention.fc_out.weight\", \"encoder.layers.1.attention.fc_out.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.1.feed_forward.0.weight\", \"encoder.layers.1.feed_forward.0.bias\", \"encoder.layers.1.feed_forward.2.weight\", \"encoder.layers.1.feed_forward.2.bias\", \"encoder.layers.2.attention.values.weight\", \"encoder.layers.2.attention.keys.weight\", \"encoder.layers.2.attention.query.weight\", \"encoder.layers.2.attention.fc_out.weight\", \"encoder.layers.2.attention.fc_out.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.2.feed_forward.0.weight\", \"encoder.layers.2.feed_forward.0.bias\", \"encoder.layers.2.feed_forward.2.weight\", \"encoder.layers.2.feed_forward.2.bias\", \"encoder.layers.3.attention.values.weight\", \"encoder.layers.3.attention.keys.weight\", \"encoder.layers.3.attention.query.weight\", \"encoder.layers.3.attention.fc_out.weight\", \"encoder.layers.3.attention.fc_out.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"encoder.layers.3.feed_forward.0.weight\", \"encoder.layers.3.feed_forward.0.bias\", \"encoder.layers.3.feed_forward.2.weight\", \"encoder.layers.3.feed_forward.2.bias\", \"encoder.layers.4.attention.values.weight\", \"encoder.layers.4.attention.keys.weight\", \"encoder.layers.4.attention.query.weight\", \"encoder.layers.4.attention.fc_out.weight\", \"encoder.layers.4.attention.fc_out.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.4.feed_forward.0.weight\", \"encoder.layers.4.feed_forward.0.bias\", \"encoder.layers.4.feed_forward.2.weight\", \"encoder.layers.4.feed_forward.2.bias\", \"encoder.layers.5.attention.values.weight\", \"encoder.layers.5.attention.keys.weight\", \"encoder.layers.5.attention.query.weight\", \"encoder.layers.5.attention.fc_out.weight\", \"encoder.layers.5.attention.fc_out.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"encoder.layers.5.feed_forward.0.weight\", \"encoder.layers.5.feed_forward.0.bias\", \"encoder.layers.5.feed_forward.2.weight\", \"encoder.layers.5.feed_forward.2.bias\", \"fc_out.weight\", \"fc_out.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 3. Load the checkpoint\u001b[39;00m\n\u001b[1;32m     23\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(SAVE_PATH, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2589\u001b[0m             ),\n\u001b[1;32m   2590\u001b[0m         )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2596\u001b[0m         )\n\u001b[1;32m   2597\u001b[0m     )\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERT:\n\tMissing key(s) in state_dict: \"encoder.word_embedding.weight\", \"encoder.position_embedding.weight\", \"encoder.layers.0.attention.values.weight\", \"encoder.layers.0.attention.keys.weight\", \"encoder.layers.0.attention.query.weight\", \"encoder.layers.0.attention.fc_out.weight\", \"encoder.layers.0.attention.fc_out.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.0.feed_forward.0.weight\", \"encoder.layers.0.feed_forward.0.bias\", \"encoder.layers.0.feed_forward.2.weight\", \"encoder.layers.0.feed_forward.2.bias\", \"encoder.layers.1.attention.values.weight\", \"encoder.layers.1.attention.keys.weight\", \"encoder.layers.1.attention.query.weight\", \"encoder.layers.1.attention.fc_out.weight\", \"encoder.layers.1.attention.fc_out.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.1.feed_forward.0.weight\", \"encoder.layers.1.feed_forward.0.bias\", \"encoder.layers.1.feed_forward.2.weight\", \"encoder.layers.1.feed_forward.2.bias\", \"encoder.layers.2.attention.values.weight\", \"encoder.layers.2.attention.keys.weight\", \"encoder.layers.2.attention.query.weight\", \"encoder.layers.2.attention.fc_out.weight\", \"encoder.layers.2.attention.fc_out.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.2.feed_forward.0.weight\", \"encoder.layers.2.feed_forward.0.bias\", \"encoder.layers.2.feed_forward.2.weight\", \"encoder.layers.2.feed_forward.2.bias\", \"encoder.layers.3.attention.values.weight\", \"encoder.layers.3.attention.keys.weight\", \"encoder.layers.3.attention.query.weight\", \"encoder.layers.3.attention.fc_out.weight\", \"encoder.layers.3.attention.fc_out.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"encoder.layers.3.feed_forward.0.weight\", \"encoder.layers.3.feed_forward.0.bias\", \"encoder.layers.3.feed_forward.2.weight\", \"encoder.layers.3.feed_forward.2.bias\", \"encoder.layers.4.attention.values.weight\", \"encoder.layers.4.attention.keys.weight\", \"encoder.layers.4.attention.query.weight\", \"encoder.layers.4.attention.fc_out.weight\", \"encoder.layers.4.attention.fc_out.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.4.feed_forward.0.weight\", \"encoder.layers.4.feed_forward.0.bias\", \"encoder.layers.4.feed_forward.2.weight\", \"encoder.layers.4.feed_forward.2.bias\", \"encoder.layers.5.attention.values.weight\", \"encoder.layers.5.attention.keys.weight\", \"encoder.layers.5.attention.query.weight\", \"encoder.layers.5.attention.fc_out.weight\", \"encoder.layers.5.attention.fc_out.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"encoder.layers.5.feed_forward.0.weight\", \"encoder.layers.5.feed_forward.0.bias\", \"encoder.layers.5.feed_forward.2.weight\", \"encoder.layers.5.feed_forward.2.bias\", \"fc_out.weight\", \"fc_out.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\". "
     ]
    }
   ],
   "source": [
    "# --- In a new script, or later in your notebook ---\n",
    "\n",
    "# 1. Define the model architecture again (or import it)\n",
    "# Make sure all your classes like SelfAttention, TransformerBlock, Encoder, and BERT are defined.\n",
    "\n",
    "# 2. Instantiate the model with the SAME hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters must match the saved model\n",
    "VOCAB_SIZE = 30000 \n",
    "EMBED_SIZE = 256\n",
    "NUM_LAYERS = 6\n",
    "HEADS = 8\n",
    "FORWARD_EXPANSION = 4\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Create an instance of the model\n",
    "encoder = Encoder(VOCAB_SIZE, EMBED_SIZE, NUM_LAYERS, HEADS, device, FORWARD_EXPANSION, DROPOUT, MAX_LENGTH)\n",
    "loaded_model = BERT(encoder, VOCAB_SIZE).to(device)\n",
    "\n",
    "# 3. Load the checkpoint\n",
    "checkpoint = torch.load(SAVE_PATH, map_location=device)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'The capital of India is New Delhi.'\n",
      "Shape of output embeddings: (256,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of output embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# You can get the embedding for the entire sentence from the [CLS] token (the first token)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of the [CLS] token embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_embedding\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- ROBUST METHOD TO FIND A SPECIFIC WORD ---\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Make sure you have your tokenizer loaded\n",
    "tokenizer = Tokenizer.from_file(\"wiki-bpe-tokenizer.json\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Let's get embeddings for a new sentence\n",
    "test_sentence = \"The capital of India is New Delhi.\"\n",
    "embeddings = get_embeddings(test_sentence, loaded_model, tokenizer, device)\n",
    "\n",
    "# The output shape will be (batch_size, sequence_length, embedding_size)\n",
    "print(f\"Sentence: '{test_sentence}'\")\n",
    "print(f\"Shape of output embeddings: {embeddings.shape}\")\n",
    "\n",
    "# You can get the embedding for the entire sentence from the [CLS] token (the first token)\n",
    "cls_embedding = embeddings[0, 0, :]\n",
    "print(f\"Shape of the [CLS] token embedding: {cls_embedding.shape}\")\n",
    "\n",
    "# --- ROBUST METHOD TO FIND A SPECIFIC WORD ---\n",
    "tokens = tokenizer.encode(test_sentence).tokens\n",
    "print(\"\\nGenerated tokens:\", tokens)\n",
    "\n",
    "target_word = \"India\"\n",
    "india_index = -1\n",
    "\n",
    "# Find the index of the token corresponding to the target word\n",
    "for i, token in enumerate(tokens):\n",
    "    # The token could be the word itself, or the word with a space prefix 'Ġ'\n",
    "    if target_word in token:\n",
    "        india_index = i\n",
    "        break\n",
    "\n",
    "if india_index != -1:\n",
    "    india_embedding = embeddings[0, india_index, :]\n",
    "    print(f\"Shape of the '{target_word}' token embedding: {india_embedding.shape}\")\n",
    "else:\n",
    "    # This might happen if the word is split into multiple sub-tokens\n",
    "    print(f\"Could not find a single token for the word '{target_word}'. It might be split into subwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading state_dict: Error(s) in loading state_dict for BERT:\n",
      "\tMissing key(s) in state_dict: \"encoder.word_embedding.weight\", \"encoder.position_embedding.weight\", \"encoder.layers.0.attention.values.weight\", \"encoder.layers.0.attention.keys.weight\", \"encoder.layers.0.attention.query.weight\", \"encoder.layers.0.attention.fc_out.weight\", \"encoder.layers.0.attention.fc_out.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.0.feed_forward.0.weight\", \"encoder.layers.0.feed_forward.0.bias\", \"encoder.layers.0.feed_forward.2.weight\", \"encoder.layers.0.feed_forward.2.bias\", \"encoder.layers.1.attention.values.weight\", \"encoder.layers.1.attention.keys.weight\", \"encoder.layers.1.attention.query.weight\", \"encoder.layers.1.attention.fc_out.weight\", \"encoder.layers.1.attention.fc_out.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.1.feed_forward.0.weight\", \"encoder.layers.1.feed_forward.0.bias\", \"encoder.layers.1.feed_forward.2.weight\", \"encoder.layers.1.feed_forward.2.bias\", \"encoder.layers.2.attention.values.weight\", \"encoder.layers.2.attention.keys.weight\", \"encoder.layers.2.attention.query.weight\", \"encoder.layers.2.attention.fc_out.weight\", \"encoder.layers.2.attention.fc_out.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.2.feed_forward.0.weight\", \"encoder.layers.2.feed_forward.0.bias\", \"encoder.layers.2.feed_forward.2.weight\", \"encoder.layers.2.feed_forward.2.bias\", \"encoder.layers.3.attention.values.weight\", \"encoder.layers.3.attention.keys.weight\", \"encoder.layers.3.attention.query.weight\", \"encoder.layers.3.attention.fc_out.weight\", \"encoder.layers.3.attention.fc_out.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"encoder.layers.3.feed_forward.0.weight\", \"encoder.layers.3.feed_forward.0.bias\", \"encoder.layers.3.feed_forward.2.weight\", \"encoder.layers.3.feed_forward.2.bias\", \"encoder.layers.4.attention.values.weight\", \"encoder.layers.4.attention.keys.weight\", \"encoder.layers.4.attention.query.weight\", \"encoder.layers.4.attention.fc_out.weight\", \"encoder.layers.4.attention.fc_out.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.4.feed_forward.0.weight\", \"encoder.layers.4.feed_forward.0.bias\", \"encoder.layers.4.feed_forward.2.weight\", \"encoder.layers.4.feed_forward.2.bias\", \"encoder.layers.5.attention.values.weight\", \"encoder.layers.5.attention.keys.weight\", \"encoder.layers.5.attention.query.weight\", \"encoder.layers.5.attention.fc_out.weight\", \"encoder.layers.5.attention.fc_out.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"encoder.layers.5.feed_forward.0.weight\", \"encoder.layers.5.feed_forward.0.bias\", \"encoder.layers.5.feed_forward.2.weight\", \"encoder.layers.5.feed_forward.2.bias\", \"fc_out.weight\", \"fc_out.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\". \n",
      "This usually means the model architecture does not match the saved checkpoint.\n",
      "\n",
      "Indexing documents...\n",
      "  Indexed document 1/6\n",
      "  Indexed document 2/6\n",
      "  Indexed document 3/6\n",
      "  Indexed document 4/6\n",
      "  Indexed document 5/6\n",
      "  Indexed document 6/6\n",
      "\n",
      "Searching for: 'What is the capital of Japan?'\n",
      "\n",
      "Search Results:\n",
      "  - Document: 'The capital of France is Paris.', Distance: 15.1568\n",
      "  - Document: 'Beijing is the capital of China.', Distance: 15.2254\n",
      "  - Document: 'The currency of Japan is the Yen.', Distance: 15.3279\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "\n",
    "# Assume the BERT model and Encoder classes from your notebook are defined here\n",
    "# For demonstration, I'm including placeholder classes.\n",
    "# Replace these with your actual model definitions.\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "    def forward(self, x, mask):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, encoder, vocab_size):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = encoder\n",
    "    def forward(self, x, mask):\n",
    "        return self.encoder(x, mask)\n",
    "\n",
    "\n",
    "def get_embeddings(sentence, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given sentence using the provided BERT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokenizer.enable_padding(length=max_length, pad_id=tokenizer.token_to_id(\"[PAD]\"))\n",
    "    tokenizer.enable_truncation(max_length=max_length)\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor(encoding.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(encoding.attention_mask, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(input_ids, attention_mask)\n",
    "        # We'll use the embedding of the [CLS] token as the sentence representation\n",
    "        cls_embedding = outputs[0, 0, :]\n",
    "    return cls_embedding.cpu().numpy()\n",
    "\n",
    "class HNSW:\n",
    "    \"\"\"\n",
    "    Hierarchical Navigable Small World (HNSW) implementation for vector indexing.\n",
    "    \"\"\"\n",
    "    def __init__(self, m=16, ef_construction=200, ef_search=50, ml=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the HNSW index.\n",
    "\n",
    "        Args:\n",
    "            m (int): The maximum number of connections for each node.\n",
    "            ef_construction (int): The size of the dynamic list for insertion.\n",
    "            ef_search (int): The size of the dynamic list for search.\n",
    "            ml (float): The normalization factor for level generation.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.ef_construction = ef_construction\n",
    "        self.ef_search = ef_search\n",
    "        self.ml = ml\n",
    "        self.graphs = []\n",
    "        self.entry_point = None\n",
    "        self.vectors = {}\n",
    "        self.next_node_id = 0\n",
    "\n",
    "    def _distance(self, v1, v2):\n",
    "        \"\"\"Calculates the Euclidean distance between two vectors.\"\"\"\n",
    "        # Ensure inputs are numpy arrays for safety\n",
    "        v1_np = np.asarray(v1)\n",
    "        v2_np = np.asarray(v2)\n",
    "        return np.linalg.norm(v1_np - v2_np)\n",
    "\n",
    "    def _get_random_level(self):\n",
    "        \"\"\"Generates a random level for a new node.\"\"\"\n",
    "        return int(-np.log(np.random.rand()) * self.ml)\n",
    "\n",
    "    def _search_layer(self, query_vec, entry_point, num_neighbors, layer_num):\n",
    "        \"\"\"\n",
    "        Searches for the nearest neighbors in a specific layer.\n",
    "        \"\"\"\n",
    "        if not entry_point or entry_point['id'] not in self.graphs[layer_num]:\n",
    "             return []\n",
    "        \n",
    "        visited = {entry_point['id']}\n",
    "        \n",
    "        dist_to_entry = self._distance(query_vec, self.vectors[entry_point['id']])\n",
    "        \n",
    "        candidates = [(-dist_to_entry, entry_point['id'])]\n",
    "        nearest_neighbors = [(dist_to_entry, entry_point['id'])]\n",
    "\n",
    "        while candidates:\n",
    "            dist, node_id = heapq.heappop(candidates)\n",
    "            dist = -dist\n",
    "\n",
    "            if len(nearest_neighbors) >= num_neighbors and dist > nearest_neighbors[-1][0]:\n",
    "                break\n",
    "\n",
    "            node = self.graphs[layer_num][node_id]\n",
    "            for neighbor_id in node['neighbors']:\n",
    "                if neighbor_id not in visited:\n",
    "                    visited.add(neighbor_id)\n",
    "                    d = self._distance(query_vec, self.vectors[neighbor_id])\n",
    "                    if len(nearest_neighbors) < num_neighbors or d < nearest_neighbors[-1][0]:\n",
    "                        heapq.heappush(candidates, (-d, neighbor_id))\n",
    "                        heapq.heappush(nearest_neighbors, (d, neighbor_id))\n",
    "                        if len(nearest_neighbors) > num_neighbors:\n",
    "                            heapq.heappop(nearest_neighbors)\n",
    "\n",
    "        return sorted(nearest_neighbors)\n",
    "\n",
    "    def _select_neighbors(self, query_vec, candidates, m):\n",
    "        \"\"\"\n",
    "        Selects the best neighbors from a list of candidates.\n",
    "        \"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        # Sort by distance (first element of tuple) and take the top M\n",
    "        return sorted(candidates, key=lambda c: c[0])[:m]\n",
    "\n",
    "    def add(self, vector):\n",
    "        \"\"\"\n",
    "        Adds a vector to the HNSW index.\n",
    "        \"\"\"\n",
    "        node_id = self.next_node_id\n",
    "        \n",
    "        # FIX: If the input vector is a PyTorch tensor, move it to CPU and convert to NumPy.\n",
    "        # This makes the HNSW class more robust to different input types.\n",
    "        if torch.is_tensor(vector):\n",
    "            vector = vector.cpu().numpy()\n",
    "\n",
    "        # Ensure vector is a flat numpy array of a consistent type\n",
    "        self.vectors[node_id] = np.asarray(vector, dtype=np.float32).flatten()\n",
    "        self.next_node_id += 1\n",
    "\n",
    "        level = self._get_random_level()\n",
    "\n",
    "        # Ensure graph lists are long enough\n",
    "        while len(self.graphs) <= level:\n",
    "            self.graphs.append({})\n",
    "\n",
    "        # Add the new node to all its levels in the graph\n",
    "        for i in range(level + 1):\n",
    "            self.graphs[i][node_id] = {'neighbors': []}\n",
    "\n",
    "        # If this is the first node, set it as the entry point and return\n",
    "        if self.entry_point is None:\n",
    "            self.entry_point = {'id': node_id, 'level': level}\n",
    "            return\n",
    "\n",
    "        # Start search from the top-level entry point\n",
    "        ep = self.entry_point\n",
    "\n",
    "        # Phase 1: Find the entry point for each level down to the new node's level + 1\n",
    "        for i in range(self.entry_point['level'], level, -1):\n",
    "            nearest_neighbor_list = self._search_layer(vector, ep, 1, i)\n",
    "            if not nearest_neighbor_list: break # Should not happen if graph is connected\n",
    "            ep = {'id': nearest_neighbor_list[0][1], 'level': i - 1}\n",
    "\n",
    "        # Phase 2: Insert the new node by connecting it to neighbors, from insertion level down to 0\n",
    "        for i in range(min(level, self.entry_point['level']), -1, -1):\n",
    "            # Find the ef_construction nearest neighbors at the current level\n",
    "            nearest_neighbors = self._search_layer(vector, ep, self.ef_construction, i)\n",
    "            \n",
    "            # Select the M best neighbors to connect to the new node\n",
    "            neighbors_to_connect = self._select_neighbors(vector, nearest_neighbors, self.m)\n",
    "            self.graphs[i][node_id]['neighbors'] = [n[1] for n in neighbors_to_connect]\n",
    "\n",
    "            # Connect the selected neighbors back to the new node\n",
    "            for _, neighbor_id in neighbors_to_connect:\n",
    "                neighbor_connections = self.graphs[i][neighbor_id]['neighbors']\n",
    "                \n",
    "                # Add the new node to the neighbor's connections\n",
    "                neighbor_connections.append(node_id)\n",
    "                \n",
    "                # If the neighbor now has too many connections, prune the farthest one\n",
    "                if len(neighbor_connections) > self.m:\n",
    "                    distances = [(self._distance(self.vectors[neighbor_id], self.vectors[c_id]), c_id) for c_id in neighbor_connections]\n",
    "                    self.graphs[i][neighbor_id]['neighbors'] = [c[1] for c in sorted(distances)[:self.m]]\n",
    "            \n",
    "            # Update the entry point for the next level down to be the closest node found at this level\n",
    "            if nearest_neighbors:\n",
    "                ep = {'id': nearest_neighbors[0][1], 'level': i - 1}\n",
    "\n",
    "        # If the new node is at a higher level than the current entry point, update it\n",
    "        if level > self.entry_point['level']:\n",
    "            self.entry_point = {'id': node_id, 'level': level}\n",
    "\n",
    "    def search(self, query_vec, k):\n",
    "        \"\"\"\n",
    "        Searches for the k-nearest neighbors to a query vector.\n",
    "        \"\"\"\n",
    "        # FIX: Ensure query vector is a NumPy array on the CPU.\n",
    "        if torch.is_tensor(query_vec):\n",
    "            query_vec = query_vec.cpu().numpy()\n",
    "\n",
    "        if not self.entry_point:\n",
    "            return []\n",
    "\n",
    "        ep = self.entry_point\n",
    "        # Phase 1: Greedily search from the top level down to level 1\n",
    "        for i in range(self.entry_point['level'], 0, -1):\n",
    "            nearest_neighbor_list = self._search_layer(query_vec, ep, 1, i)\n",
    "            if not nearest_neighbor_list: break\n",
    "            ep = {'id': nearest_neighbor_list[0][1], 'level': i - 1}\n",
    "\n",
    "        # Phase 2: Perform an exhaustive search at the base layer (level 0)\n",
    "        nearest_neighbors = self._search_layer(query_vec, ep, self.ef_search, 0)\n",
    "        return sorted(nearest_neighbors)[:k]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Setup ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load your trained tokenizer\n",
    "    try:\n",
    "        tokenizer = Tokenizer.from_file(\"wiki-bpe-tokenizer.json\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Tokenizer file not found. Please ensure 'wiki-bpe-tokenizer.json' is in the same directory.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- Load your trained model ---\n",
    "    # Make sure the hyperparameters match your saved model\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "    EMBED_SIZE = 256\n",
    "    NUM_LAYERS = 6\n",
    "    HEADS = 8\n",
    "    FORWARD_EXPANSION = 4\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LENGTH = 128\n",
    "\n",
    "    encoder = Encoder(VOCAB_SIZE, EMBED_SIZE, NUM_LAYERS, HEADS, device, FORWARD_EXPANSION, DROPOUT, MAX_LENGTH)\n",
    "    model = BERT(encoder, VOCAB_SIZE).to(device)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(\"bert_mlm_checkpoint.pth\", map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model checkpoint not found. Using a randomly initialized model for demonstration.\")\n",
    "\n",
    "\n",
    "    # --- Create and populate the HNSW index ---\n",
    "    hnsw_index = HNSW()\n",
    "\n",
    "    # Sample documents to be indexed\n",
    "    documents = [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"The Eiffel Tower is a famous landmark in Paris.\",\n",
    "        \"The currency of Japan is the Yen.\",\n",
    "        \"Tokyo is the largest city in Japan.\",\n",
    "        \"The Great Wall of China is an ancient wonder.\",\n",
    "        \"Beijing is the capital of China.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nIndexing documents...\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        embedding = get_embeddings(doc, model, tokenizer, device)\n",
    "        hnsw_index.add(embedding)\n",
    "        print(f\"  Indexed document {i+1}/{len(documents)}\")\n",
    "\n",
    "    # --- Perform a search ---\n",
    "    query_sentence = \"What is the capital of Japan?\"\n",
    "    query_embedding = get_embeddings(query_sentence, model, tokenizer, device)\n",
    "\n",
    "    print(f\"\\nSearching for: '{query_sentence}'\")\n",
    "    results = hnsw_index.search(query_embedding, k=3)\n",
    "\n",
    "    print(\"\\nSearch Results:\")\n",
    "    if results:\n",
    "        for dist, node_id in results:\n",
    "            # Check if node_id is valid\n",
    "            if node_id < len(documents):\n",
    "                print(f\"  - Document: '{documents[node_id]}', Distance: {dist:.4f}\")\n",
    "            else:\n",
    "                print(f\"  - Invalid node_id found: {node_id}\")\n",
    "    else:\n",
    "        print(\"  No results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1befa314232048f2a7265c280248e789": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26e9aafa04e54383b4682b75cf67d3df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1befa314232048f2a7265c280248e789",
      "placeholder": "​",
      "style": "IPY_MODEL_f1b194374a3240d8aab759ddef189ad6",
      "value": " 41/41 [00:00&lt;00:00, 42.14it/s]"
     }
    },
    "2e0552c12c0e4598ba8c6eaf2745e936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d268814ff3f4685a3a29c281d11e71b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7940d250af4246b2b1d42f3b7cf3f3a1",
      "placeholder": "​",
      "style": "IPY_MODEL_e21599a6e38844e39915c89cfaeaed95",
      "value": "Resolving data files: 100%"
     }
    },
    "5c9f0a73169f47a1841d7ed70175f80b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e0552c12c0e4598ba8c6eaf2745e936",
      "max": 41,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9ff2ad176614d9fbbe83347e86a313b",
      "value": 41
     }
    },
    "7940d250af4246b2b1d42f3b7cf3f3a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92ff4a467f1e48de8c9cc891853af457": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d268814ff3f4685a3a29c281d11e71b",
       "IPY_MODEL_5c9f0a73169f47a1841d7ed70175f80b",
       "IPY_MODEL_26e9aafa04e54383b4682b75cf67d3df"
      ],
      "layout": "IPY_MODEL_b7d9cf86760c41fbacae8ed644357758"
     }
    },
    "b7d9cf86760c41fbacae8ed644357758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e21599a6e38844e39915c89cfaeaed95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ff2ad176614d9fbbe83347e86a313b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1b194374a3240d8aab759ddef189ad6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
